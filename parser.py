# parser_multi.py
import os
import json
import re
import time
from typing import Optional, Tuple, List, Dict

import requests
from bs4 import BeautifulSoup
from loguru import logger

# -------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ----------
STATE_FILE = "last_links.json"
REQUEST_TIMEOUT = 15
HEADERS = {
    "User-Agent": "Mozilla/5.0",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "tr-TR,tr;q=0.9,en;q=0.8,ru;q=0.7",
    "Connection": "keep-alive",
}

# -------- Telegram ----------
TELEGRAM_TOKEN = os.getenv('TELEGRAM_TOKEN')
TELEGRAM_CHAT_ID = os.getenv('TELEGRAM_CHAT_ID')

if not TELEGRAM_TOKEN or not TELEGRAM_CHAT_ID:
    logger.error("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç TELEGRAM_TOKEN –∏–ª–∏ TELEGRAM_CHAT_ID.")
    raise SystemExit(1)

def send_to_telegram(text: str) -> bool:
    url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
    payload = {
        "chat_id": TELEGRAM_CHAT_ID,
        "text": text,
        "parse_mode": "HTML",
        "disable_web_page_preview": False
    }
    try:
        response = requests.post(url, data=payload, timeout=10)
        if response.status_code == 200:
            logger.success("‚úÖ –°–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ Telegram.")
            return True
        logger.error(f"Telegram –æ—à–∏–±–∫–∞: {response.status_code} - {response.text}")
        return False
    except requests.RequestException as e:
        logger.error(f"Telegram —Å–µ—Ç—å: {e}")
        return False

def format_for_telegram(text: str, title: str, url: str) -> str:
    # –ß–∏—Å—Ç–∏–º —Ç—è–∂—ë–ª—ã–µ —Ç–µ–≥–∏ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π
    clean = re.sub(
        r'</?(h\d|div|span|table|tr|td|style|script|figure|img|source|picture)[^>]*>',
        '', text, flags=re.I
    ).strip()
    message = (
        f"<b>{title}</b>\n\n"
        f"{clean}\n\n"
        f"–ò—Å—Ç–æ—á–Ω–∏–∫: {url}\n"
        f"<a href='{url}'>Kaynak</a>"
    )
    # Telegram –ª–∏–º–∏—Ç 4096 —Å–∏–º–≤–æ–ª–æ–≤
    return message[:4096]

# -------- –°–æ—Å—Ç–æ—è–Ω–∏–µ ----------
def load_state() -> Dict[str, str]:
    if not os.path.exists(STATE_FILE):
        return {}
    try:
        with open(STATE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å {STATE_FILE}: {e}")
        return {}

def save_state(state: Dict[str, str]) -> None:
    try:
        with open(STATE_FILE, "w", encoding="utf-8") as f:
            json.dump(state, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø–∏—Å–∞—Ç—å {STATE_FILE}: {e}")

# =======================
# Ajansspor
# =======================
def parse_ajansspor_latest_news(base_url: str) -> Optional[Tuple[str, str]]:
    logger.info(f"üîç Ajansspor –ø–∞—Ä—Å–∏–Ω–≥: {base_url}")
    try:
        response = requests.get(base_url, timeout=REQUEST_TIMEOUT, headers=HEADERS)
        response.raise_for_status()
    except requests.RequestException as e:
        logger.error(f"Ajansspor: –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
        return None

    soup = BeautifulSoup(response.content, 'html.parser')

    # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –ø–µ—Ä–≤—É—é –∫–∞—Ä—Ç–æ—á–∫—É/—Å–ª–∞–π–¥ —Å –Ω–æ–≤–æ—Å—Ç—å—é
    first_card = soup.find('div', class_='card') \
                 or soup.find('article') \
                 or soup.select_one('.news-card, .slider-item, .list-item')

    if not first_card:
        logger.warning("Ajansspor: –Ω–µ –Ω–∞–π–¥–µ–Ω –±–ª–æ–∫ –Ω–æ–≤–æ—Å—Ç–∏.")
        return None

    link_tag = first_card.find('a', href=True)
    if not link_tag:
        logger.warning("Ajansspor: –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞.")
        return None

    href = link_tag['href']
    if href.startswith('http'):
        news_link = href
    else:
        news_link = f"https://ajansspor.com{href}"

    details = get_ajansspor_news_details(news_link)
    if not details:
        return None
    message, url = details
    return message, url

def get_ajansspor_news_details(news_url: str) -> Optional[Tuple[str, str]]:
    logger.info(f"üìÑ Ajansspor —Å—Ç–∞—Ç—å—è: {news_url}")
    try:
        resp = requests.get(news_url, timeout=REQUEST_TIMEOUT, headers=HEADERS)
        resp.raise_for_status()
    except requests.RequestException as e:
        logger.error(f"Ajansspor: –æ—à–∏–±–∫–∞ —Å—Ç–∞—Ç—å–∏: {e}")
        return None

    soup = BeautifulSoup(resp.content, 'html.parser')

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    title_tag = soup.find('header', class_='news-header') \
                or soup.find('h1') \
                or soup.find('title')
    title = title_tag.get_text(strip=True) if title_tag else "Ba≈ülƒ±ksƒ±z"

    # –ö–æ–Ω—Ç–µ–Ω—Ç
    content: List[str] = []
    # –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã, –∑–∞—â–∏—â–∞–µ–º—Å—è –æ—Ç –≤–∞—Ä–∏–∞—Ü–∏–π –≤–µ—Ä—Å—Ç–∫–∏
    for block in soup.select('div.article-content, div.news-detail, article, main, section'):
        # –ó–∞–≥–æ–ª–æ–≤–∫–∏
        for h in block.find_all(re.compile(r'^h[1-6]$')):
            t = h.get_text(strip=True)
            if t:
                content.append(t)

        # –ê–±–∑–∞—Ü—ã
        for p in block.find_all('p'):
            p_text = p.get_text(" ", strip=True)
            if p_text:
                content.append(p_text)

    # –§–æ–ª–±—ç–∫ ‚Äî –≤—Å–µ <p>
    if not content:
        for p in soup.find_all('p'):
            t = p.get_text(" ", strip=True)
            if t:
                content.append(t)

    full_text = "\n\n".join(content).strip()
    if not full_text:
        logger.warning("‚ùå Ajansspor: —Å—Ç–∞—Ç—å—è –ø—É—Å—Ç–∞.")
        return None

    message = format_for_telegram(full_text, title, news_url)
    return message, news_url

# =======================
# Anadolu Ajansƒ± (AA Spor)
# =======================
AA_SPORTS_URL = "https://www.aa.com.tr/tr/spor"
AA_SPORTS_RSS = "https://www.aa.com.tr/tr/rss/default?cat=spor"

AA_BLACKLIST_SNIPPETS = [
    "AA'nƒ±n WhatsApp kanallarƒ±na",
    "Bu haberi payla≈üƒ±n",
    "AA Haber Akƒ±≈ü Sistemi (HAS)",
    "Anadolu Ajansƒ± web sitesinde",
]

# –†–∞–∑—Ä–µ—à–∞–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏, –Ω–µ—Å–∫–æ–ª—å–∫–æ ¬´—Å–ø–æ—Ä—Ç¬ª-—Ä–∞–∑–¥–µ–ª–æ–≤
AA_INDEX_LINK_RE = re.compile(
    r"^(?:https?://(?:www\.)?aa\.com\.tr)?"
    r"/tr/(?:spor|futbol|basketbol|voleybol|dunyadan-spor)"
    r"/[a-z0-9\-]+/\d+/?$",
    re.I
)

def _aa_rss_latest_link() -> Optional[str]:
    """–ë–µ—Ä—ë–º —Å–∞–º—É—é —Å–≤–µ–∂—É—é —Å—Å—ã–ª–∫—É –∏–∑ RSS —Å–ø–æ—Ä—Ç–∞ AA (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)."""
    try:
        r = requests.get(AA_SPORTS_RSS, timeout=REQUEST_TIMEOUT, headers=HEADERS)
        r.raise_for_status()
    except requests.RequestException as e:
        logger.error(f"AA RSS –∑–∞–≥—Ä—É–∑–∫–∞: {e}")
        return None

    # RSS —É AA –º–æ–∂–µ—Ç –±—ã—Ç—å –≤ XML –∏–ª–∏ Atom-–≤–∞—Ä–∏–∞—Ü–∏–∏
    soup = BeautifulSoup(r.content, "xml")

    item = soup.find("item")
    if item and item.find("link"):
        link = item.find("link").get_text(strip=True)
        return link

    entry = soup.find("entry")
    if entry:
        link = entry.find("link")
        if link and link.get("href"):
            return link.get("href").strip()
    return None

def _aa_pick_first_from_index() -> Optional[str]:
    """–§–æ–ª–±—ç–∫: –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ —Å–ø–æ—Ä—Ç–∞ –±–µ—Ä—ë–º –ø–µ—Ä–≤—É—é –≤–∞–ª–∏–¥–Ω—É—é —Å—Å—ã–ª–∫—É –Ω–∞ –Ω–æ–≤–æ—Å—Ç—å."""
    try:
        resp = requests.get(AA_SPORTS_URL, timeout=REQUEST_TIMEOUT, headers=HEADERS)
        resp.raise_for_status()
    except requests.RequestException as e:
        logger.error(f"AA index –∑–∞–≥—Ä—É–∑–∫–∞: {e}")
        return None

    soup = BeautifulSoup(resp.content, "html.parser")

    # 1) –°—Ç—Ä–æ–≥–∏–π –ø–æ–∏—Å–∫ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É
    for a in soup.find_all("a", href=True):
        href = a["href"].strip()
        if AA_INDEX_LINK_RE.match(href):
            full = href if href.startswith("http") else "https://www.aa.com.tr" + href
            logger.debug(f"AA match (strict): {full}")
            return full

    # 2) –§–æ–ª–±—ç–∫: –ª—é–±—ã–µ —Å—Å—ã–ª–∫–∏ —Å–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π /tr/.../<id> –∏ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º 'spor'
    for a in soup.find_all("a", href=True):
        href = a["href"].strip()
        if "/tr/" in href and re.search(r"/\d+/?$", href) and "spor" in href:
            full = href if href.startswith("http") else "https://www.aa.com.tr" + href
            logger.debug(f"AA match (fallback): {full}")
            return full

    # 3) –ï—â—ë –æ–¥–∏–Ω —Ñ–æ–ª–±—ç–∫: –ø—Ä–æ–±—É–µ–º –≤—ã—Ç–∞—â–∏—Ç—å –∏–∑ –∫–∞—Ä—Ç–æ—á–µ–∫
    for sel in (".news", ".card", "article", ".list", ".content", ".headline"):
        for a in soup.select(f"{sel} a[href]"):
            href = a.get("href", "").strip()
            if AA_INDEX_LINK_RE.match(href):
                full = href if href.startswith("http") else "https://www.aa.com.tr" + href
                logger.debug(f"AA match (selector {sel}): {full}")
                return full

    logger.warning("AA: –Ω–∞ –∏–Ω–¥–µ–∫—Å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ–¥—Ö–æ–¥—è—â—É—é —Å—Å—ã–ª–∫—É.")
    return None

def get_aa_article(news_url: str) -> Optional[Tuple[str, str]]:
    """–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Å—Ç–∞—Ç—å—è AA: –±–µ—Ä—ë–º –≤—Å–µ H1..H6 –∏ P –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞."""
    logger.info(f"üìÑ AA —Å—Ç–∞—Ç—å—è: {news_url}")
    try:
        resp = requests.get(news_url, timeout=REQUEST_TIMEOUT, headers=HEADERS)
        resp.raise_for_status()
    except requests.RequestException as e:
        logger.error(f"AA —Å—Ç–∞—Ç—å—è: {e}")
        return None

    soup = BeautifulSoup(resp.content, "html.parser")

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫: <h1> -> og:title -> <title>
    title = None
    h1 = soup.find("h1")
    if h1 and h1.get_text(strip=True):
        title = h1.get_text(strip=True)
    if not title:
        og = soup.find("meta", property="og:title")
        if og and og.get("content"):
            title = og["content"].strip()
    if not title:
        t = soup.find("title")
        title = t.get_text(strip=True) if t else "Ba≈ülƒ±ksƒ±z"

    # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å—Ç–∞—Ç—å–∏ ‚Äî —É—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –≤–µ—Ä—Å—Ç–∫–∏
    candidates = [
        "div.detay-icerik",
        "div.print",
        'div[itemprop="articleBody"]',
        "div.article",
        "article",
        "main",
        "section.content",
        "section#content",
    ]
    body = None
    for sel in candidates:
        node = soup.select_one(sel)
        if node and (node.find(re.compile(r'^h[1-6]$')) or node.find("p")):
            body = node
            break
    if body is None:
        body = soup  # –∫—Ä–∞–π–Ω–∏–π —Å–ª—É—á–∞–π ‚Äî —Å–º–æ—Ç—Ä–∏–º –≤–µ—Å—å –¥–æ–∫—É–º–µ–Ω—Ç

    parts: List[str] = []
    for tag in body.find_all(["h1","h2","h3","h4","h5","h6","p"], recursive=True):
        # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —è–≤–Ω—ã–µ —Å–æ—Ü/–ø—Ä–æ–º–æ/–±–æ–∫–æ–≤—ã–µ –±–ª–æ–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º
        classes = " ".join(tag.get("class", [])) if tag.has_attr("class") else ""
        if re.search(r"(share|sosyal|whatsapp|promo|related|cookie|benzerHaberler|subscription|banner)", classes, re.I):
            continue
        text = tag.get_text(" ", strip=True)
        if not text:
            continue
        # –æ—Ç—Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø—Ä–æ–º–æ-—Ñ—Ä–∞–∑—ã
        if any(bad.lower() in text.lower() for bad in AA_BLACKLIST_SNIPPETS):
            continue
        parts.append(text)

    # –§–æ–ª–±—ç–∫: –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ —Å–æ–±—Ä–∞–ª–æ—Å—å ‚Äî –≤–æ–∑—å–º—ë–º –≤—Å–µ <p> –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É
    if not parts:
        for p in soup.find_all("p"):
            t = p.get_text(" ", strip=True)
            if t and not any(bad.lower() in t.lower() for bad in AA_BLACKLIST_SNIPPETS):
                parts.append(t)

    full_text = "\n\n".join(parts).strip()
    if not full_text:
        logger.warning("AA: —Å—Ç–∞—Ç—å—è –ø—É—Å—Ç–∞.")
        return None

    message = format_for_telegram(full_text, title, news_url)
    return message, news_url

def parse_aa_spor_latest() -> Optional[Tuple[str, str]]:
    """–ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –Ω–æ–≤–æ—Å—Ç—å AA (RSS -> fallback –Ω–∞ –∏–Ω–¥–µ–∫—Å), –≤—Ö–æ–¥–∏–º –≤–Ω—É—Ç—Ä—å –∏ –ø–∞—Ä—Å–∏–º H+P."""
    link = _aa_rss_latest_link()
    if not link:
        logger.warning("AA RSS –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –±–µ—Ä—ë–º —Å—Å—ã–ª–∫—É —Å –∏–Ω–¥–µ–∫—Å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–ø–æ—Ä—Ç–∞.")
        link = _aa_pick_first_from_index()
    if not link:
        logger.warning("AA: –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å–≤–µ–∂—É—é —Å—Å—ã–ª–∫—É.")
        return None

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å—Å—ã–ª–∫—É (–Ω–∞ —Å–ª—É—á–∞–π –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–≥–æ href)
    if not link.startswith("http"):
        link = "https://www.aa.com.tr" + link
    return get_aa_article(link)

# -------------- –ì–ª–∞–≤–Ω–∞—è --------------
def main():
    state = load_state()  # {"ajansspor": "...", "aa_spor": "..."}
    total_sent = 0

    # Ajansspor
    try:
        ajans_url = "https://ajansspor.com/kategori/16/futbol"
        key = "ajansspor"
        res = parse_ajansspor_latest_news(ajans_url)
        if res:
            message, new_link = res
            if state.get(key) != new_link:
                logger.info(f"üöÄ Ajansspor –Ω–æ–≤–∞—è: {new_link}")
                if send_to_telegram(message):
                    state[key] = new_link
                    save_state(state)
                    total_sent += 1
            else:
                logger.info("‚ôªÔ∏è Ajansspor —É–∂–µ –ø—É–±–ª–∏–∫–æ–≤–∞–ª–∞—Å—å.")
        else:
            logger.info("üì≠ Ajansspor: –Ω–æ–≤–æ—Å—Ç—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
    except Exception as e:
        logger.exception(f"Ajansspor: –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")

    # AA Spor
    try:
        key = "aa_spor"
        res = parse_aa_spor_latest()
        if res:
            message, new_link = res
            if state.get(key) != new_link:
                logger.info(f"üöÄ AA –Ω–æ–≤–∞—è: {new_link}")
                if send_to_telegram(message):
                    state[key] = new_link
                    save_state(state)
                    total_sent += 1
            else:
                logger.info("‚ôªÔ∏è AA —É–∂–µ –ø—É–±–ª–∏–∫–æ–≤–∞–ª–∞—Å—å.")
        else:
            logger.info("üì≠ AA: –Ω–æ–≤–æ—Å—Ç—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
    except Exception as e:
        logger.exception(f"AA: –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")

    if total_sent == 0:
        logger.info("üì≠ –ù–µ—Ç —Å–≤–µ–∂–∏—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏–π.")
    else:
        logger.success(f"‚úÖ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π: {total_sent}")

if __name__ == "__main__":
    # –ü–æ–ª–µ–∑–Ω–æ –≤–∏–¥–µ—Ç—å –æ—Ç–ª–∞–¥–∫—É –ø—Ä–∏ –ø–µ—Ä–≤—ã—Ö –∑–∞–ø—É—Å–∫–∞—Ö:
    logger.remove()
    logger.add(lambda msg: print(msg, end=""), level="DEBUG")
    main()

# parser_multi.py
import os
import json
import re
import time
from typing import Optional, Tuple, List, Dict

import requests
from bs4 import BeautifulSoup
from loguru import logger

# -------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ----------
STATE_FILE = "last_links.json"
REQUEST_TIMEOUT = 15
HEADERS = {"User-Agent": "Mozilla/5.0"}

# -------- Telegram ----------
TELEGRAM_TOKEN = os.getenv('TELEGRAM_TOKEN')
TELEGRAM_CHAT_ID = os.getenv('TELEGRAM_CHAT_ID')

if not TELEGRAM_TOKEN or not TELEGRAM_CHAT_ID:
    logger.error("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç TELEGRAM_TOKEN –∏–ª–∏ TELEGRAM_CHAT_ID.")
    raise SystemExit(1)

def send_to_telegram(text: str) -> bool:
    url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
    payload = {
        "chat_id": TELEGRAM_CHAT_ID,
        "text": text,
        "parse_mode": "HTML",
        "disable_web_page_preview": False
    }
    try:
        response = requests.post(url, data=payload, timeout=10)
        if response.status_code == 200:
            logger.success("‚úÖ –°–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ Telegram.")
            return True
        logger.error(f"Telegram –æ—à–∏–±–∫–∞: {response.status_code} - {response.text}")
        return False
    except requests.RequestException as e:
        logger.error(f"Telegram —Å–µ—Ç—å: {e}")
        return False

def format_for_telegram(text: str, title: str, url: str) -> str:
    # –ß–∏—Å—Ç–∏–º —Ç—è–∂—ë–ª—ã–µ —Ç–µ–≥–∏ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π
    clean = re.sub(
        r'</?(h\d|div|span|table|tr|td|style|script|figure|img|source|picture)[^>]*>',
        '', text, flags=re.I
    ).strip()
    message = (
        f"<b>{title}</b>\n\n"
        f"{clean}\n\n"
        f"–ò—Å—Ç–æ—á–Ω–∏–∫: {url}\n"
        f"<a href='{url}'>Kaynak</a>"
    )
    return message[:4096]  # –ª–∏–º–∏—Ç Telegram

# -------- –°–æ—Å—Ç–æ—è–Ω–∏–µ ----------
def load_state() -> Dict[str, str]:
    if not os.path.exists(STATE_FILE):
        return {}
    try:
        with open(STATE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å {STATE_FILE}: {e}")
        return {}

def save_state(state: Dict[str, str]) -> None:
    with open(STATE_FILE, "w", encoding="utf-8") as f:
        json.dump(state, f, ensure_ascii=False, indent=2)

# =======================
# Ajansspor (–∫–∞–∫ —É —Ç–µ–±—è)
# =======================
def parse_ajansspor_latest_news(base_url: str) -> Optional[Tuple[str, str]]:
    logger.info(f"üîç Ajansspor –ø–∞—Ä—Å–∏–Ω–≥: {base_url}")
    try:
        response = requests.get(base_url, timeout=REQUEST_TIMEOUT, headers=HEADERS)
        response.raise_for_status()
    except requests.RequestException as e:
        logger.error(f" –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
        return None

    soup = BeautifulSoup(response.content, 'html.parser')
    first_card = soup.find('div', class_='card')
    if not first_card:
        logger.warning(" Ajansspor: –Ω–µ –Ω–∞–π–¥–µ–Ω –±–ª–æ–∫ –Ω–æ–≤–æ—Å—Ç–∏.")
        return None

    link_tag = first_card.find('a', href=True)
    if not link_tag:
        logger.warning(" Ajansspor: –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞.")
        return None

    news_link = f"https://ajansspor.com{link_tag['href']}"
    details = get_ajansspor_news_details(news_link)
    if not details:
        return None
    message, url = details
    return message, url

def get_ajansspor_news_details(news_url: str) -> Optional[Tuple[str, str]]:
    logger.info(f"üìÑ Ajansspor —Å—Ç–∞—Ç—å—è: {news_url}")
    try:
        resp = requests.get(news_url, timeout=REQUEST_TIMEOUT, headers=HEADERS)
        resp.raise_for_status()
    except requests.RequestException as e:
        logger.error(f" –û—à–∏–±–∫–∞ —Å—Ç–∞—Ç—å–∏: {e}")
        return None

    soup = BeautifulSoup(resp.content, 'html.parser')
    title_tag = soup.find('header', class_='news-header')
    title = title_tag.get_text(strip=True) if title_tag else "Ba≈ülƒ±ksƒ±z"

    content: List[str] = []
    for block in soup.find_all('div', class_='article-content'):
        detail = block.find('div', class_='news-detail')
        if not detail:
            continue

        # –≤—Å–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ h-—Ç–µ–≥–æ–≤ –≤–Ω—É—Ç—Ä–∏ detail
        for h in detail.find_all(re.compile(r'^h[1-6]$')):
            t = h.get_text(strip=True)
            if t:
                content.append(t)

        # –∞–±–∑–∞—Ü—ã –≤–Ω—É—Ç—Ä–∏ <article>
        article_tag = detail.find('article')
        if article_tag:
            for p in article_tag.find_all('p'):
                p_text = p.get_text(strip=True)
                if p_text:
                    content.append(p_text)

    full_text = "\n\n".join(content).strip()
    if not full_text:
        logger.warning("‚ùå Ajansspor: —Å—Ç–∞—Ç—å—è –ø—É—Å—Ç–∞.")
        return None

    message = format_for_telegram(full_text, title, news_url)
    return message, news_url

# =======================
# Anadolu Ajansƒ± (AA Spor)
# =======================
AA_SPORTS_URL = "https://www.aa.com.tr/tr/spor"
AA_SPORTS_RSS = "https://www.aa.com.tr/tr/rss/default?cat=spor"

AA_BLACKLIST_SNIPPETS = [
    "AA'nƒ±n WhatsApp kanallarƒ±na",  # –ø—Ä–æ–º–æ-–±–ª–æ–∫
    "Bu haberi payla≈üƒ±n",
    "AA Haber Akƒ±≈ü Sistemi (HAS)",
    "Anadolu Ajansƒ± web sitesinde",
]

def _aa_rss_latest_link() -> Optional[str]:
    """–ë–µ—Ä—ë–º —Å–∞–º—É—é —Å–≤–µ–∂—É—é —Å—Å—ã–ª–∫—É –∏–∑ RSS —Å–ø–æ—Ä—Ç–∞ AA (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)."""
    try:
        r = requests.get(AA_SPORTS_RSS, timeout=REQUEST_TIMEOUT, headers=HEADERS)
        r.raise_for_status()
    except requests.RequestException as e:
        logger.error(f"AA RSS –∑–∞–≥—Ä—É–∑–∫–∞: {e}")
        return None

    soup = BeautifulSoup(r.content, "xml")
    item = soup.find("item")
    if item and item.find("link"):
        return item.find("link").get_text(strip=True)
    entry = soup.find("entry")
    if entry:
        link = entry.find("link")
        if link and link.get("href"):
            return link.get("href").strip()
    return None

def _aa_pick_first_from_index() -> Optional[str]:
    """–§–æ–ª–±—ç–∫: –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ —Å–ø–æ—Ä—Ç–∞ –±–µ—Ä—ë–º –ø–µ—Ä–≤—É—é –≤–∞–ª–∏–¥–Ω—É—é —Å—Å—ã–ª–∫—É –Ω–∞ –Ω–æ–≤–æ—Å—Ç—å."""
    try:
        resp = requests.get(AA_SPORTS_URL, timeout=REQUEST_TIMEOUT, headers=HEADERS)
        resp.raise_for_status()
    except requests.RequestException as e:
        logger.error(f"AA index –∑–∞–≥—Ä—É–∑–∫–∞: {e}")
        return None

    soup = BeautifulSoup(resp.content, "html.parser")
    pattern = re.compile(r"^/tr/(spor|futbol|basketbol|voleybol|dunyadan-spor)/[a-z0-9\-]+/\d+$", re.I)
    for a in soup.find_all("a", href=True):
        href = a["href"]
        if pattern.match(href):
            return "https://www.aa.com.tr" + href
    return None

def get_aa_article(news_url: str) -> Optional[Tuple[str, str]]:
    """–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Å—Ç–∞—Ç—å—è AA: –±–µ—Ä—ë–º –≤—Å–µ H1..H6 –∏ P –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞."""
    logger.info(f"üìÑ AA —Å—Ç–∞—Ç—å—è: {news_url}")
    try:
        resp = requests.get(news_url, timeout=REQUEST_TIMEOUT, headers=HEADERS)
        resp.raise_for_status()
    except requests.RequestException as e:
        logger.error(f"AA —Å—Ç–∞—Ç—å—è: {e}")
        return None

    soup = BeautifulSoup(resp.content, "html.parser")

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫: <h1> -> og:title -> <title>
    title = None
    h1 = soup.find("h1")
    if h1 and h1.get_text(strip=True):
        title = h1.get_text(strip=True)
    if not title:
        og = soup.find("meta", property="og:title")
        if og and og.get("content"):
            title = og["content"].strip()
    if not title:
        t = soup.find("title")
        title = t.get_text(strip=True) if t else "Ba≈ülƒ±ksƒ±z"

    # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å—Ç–∞—Ç—å–∏ (–ø–æ —Ç–≤–æ–µ–º—É –ø—Ä–∏–º–µ—Ä—É: .print > .detay-icerik)
    candidates = [
        "div.detay-icerik",
        "div.print",
        'div[itemprop="articleBody"]',
        "article",
        "main",
        "section.content",
    ]
    body = None
    for sel in candidates:
        node = soup.select_one(sel)
        if node and (node.find(re.compile(r'^h[1-6]$')) or node.find("p")):
            body = node
            break
    if body is None:
        body = soup  # –∫—Ä–∞–π–Ω–∏–π —Å–ª—É—á–∞–π ‚Äî —Å–º–æ—Ç—Ä–∏–º –≤–µ—Å—å –¥–æ–∫—É–º–µ–Ω—Ç

    parts: List[str] = []
    for tag in body.find_all(["h1","h2","h3","h4","h5","h6","p"], recursive=True):
        # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —è–≤–Ω—ã–µ —Å–æ—Ü/–ø—Ä–æ–º–æ/–±–æ–∫–æ–≤—ã–µ –±–ª–æ–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º
        classes = " ".join(tag.get("class", [])) if tag.has_attr("class") else ""
        if re.search(r"(share|sosyal|whatsapp|promo|related|cookie|benzerHaberler)", classes, re.I):
            continue
        text = tag.get_text(" ", strip=True)
        if not text:
            continue
        # –æ—Ç—Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø—Ä–æ–º–æ-—Ñ—Ä–∞–∑—ã
        if any(bad.lower() in text.lower() for bad in AA_BLACKLIST_SNIPPETS):
            continue
        parts.append(text)

    # –§–æ–ª–±—ç–∫: –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ —Å–æ–±—Ä–∞–ª–æ—Å—å ‚Äî –≤–æ–∑—å–º—ë–º –≤—Å–µ <p>
    if not parts:
        for p in soup.find_all("p"):
            t = p.get_text(" ", strip=True)
            if t and not any(bad.lower() in t.lower() for bad in AA_BLACKLIST_SNIPPETS):
                parts.append(t)

    full_text = "\n\n".join(parts).strip()
    if not full_text:
        logger.warning("AA: —Å—Ç–∞—Ç—å—è –ø—É—Å—Ç–∞.")
        return None

    message = format_for_telegram(full_text, title, news_url)
    return message, news_url

def parse_aa_spor_latest() -> Optional[Tuple[str, str]]:
    """–ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –Ω–æ–≤–æ—Å—Ç—å AA (RSS -> fallback –Ω–∞ –∏–Ω–¥–µ–∫—Å), –≤—Ö–æ–¥–∏–º –≤–Ω—É—Ç—Ä—å –∏ –ø–∞—Ä—Å–∏–º H+P."""
    link = _aa_rss_latest_link()
    if not link:
        logger.warning("AA RSS –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –±–µ—Ä—ë–º —Å—Å—ã–ª–∫—É —Å –∏–Ω–¥–µ–∫—Å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–ø–æ—Ä—Ç–∞.")
        link = _aa_pick_first_from_index()
    if not link:
        logger.warning("AA: –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å–≤–µ–∂—É—é —Å—Å—ã–ª–∫—É.")
        return None
    return get_aa_article(link)

# -------------- –ì–ª–∞–≤–Ω–∞—è --------------
def main():
    state = load_state()  # {"ajansspor": "...", "aa_spor": "..."}
    total_sent = 0

    # Ajansspor
    try:
        ajans_url = "https://ajansspor.com/kategori/16/futbol"
        key = "ajansspor"
        res = parse_ajansspor_latest_news(ajans_url)
        if res:
            message, new_link = res
            if state.get(key) != new_link:
                logger.info(f"üöÄ Ajansspor –Ω–æ–≤–∞—è: {new_link}")
                if send_to_telegram(message):
                    state[key] = new_link
                    save_state(state)
                    total_sent += 1
            else:
                logger.info("‚ôªÔ∏è Ajansspor —É–∂–µ –ø—É–±–ª–∏–∫–æ–≤–∞–ª–∞—Å—å.")
        else:
            logger.info("üì≠ Ajansspor: –Ω–æ–≤–æ—Å—Ç—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
    except Exception as e:
        logger.exception(f"Ajansspor: –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")

    # AA Spor
    try:
        key = "aa_spor"
        res = parse_aa_spor_latest()
        if res:
            message, new_link = res
            if state.get(key) != new_link:
                logger.info(f"üöÄ AA –Ω–æ–≤–∞—è: {new_link}")
                if send_to_telegram(message):
                    state[key] = new_link
                    save_state(state)
                    total_sent += 1
            else:
                logger.info("‚ôªÔ∏è AA —É–∂–µ –ø—É–±–ª–∏–∫–æ–≤–∞–ª–∞—Å—å.")
        else:
            logger.info("üì≠ AA: –Ω–æ–≤–æ—Å—Ç—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
    except Exception as e:
        logger.exception(f"AA: –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")

    if total_sent == 0:
        logger.info("üì≠ –ù–µ—Ç —Å–≤–µ–∂–∏—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏–π.")
    else:
        logger.success(f"‚úÖ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π: {total_sent}")

if __name__ == "__main__":
    main()

import os
import requests
from bs4 import BeautifulSoup
from loguru import logger
import re

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

# --- Telegram –∫–æ–Ω—Ñ–∏–≥ ---
TELEGRAM_TOKEN = os.getenv('TELEGRAM_TOKEN')
TELEGRAM_CHAT_ID = os.getenv('TELEGRAM_CHAT_ID')

if not TELEGRAM_TOKEN or not TELEGRAM_CHAT_ID:
    logger.error("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è TELEGRAM_TOKEN –∏–ª–∏ TELEGRAM_CHAT_ID.")
    exit()

# --- –ó–∞–≥—Ä—É–∑–∫–∞ NLP-–º–æ–¥–µ–ª–∏ –¥–ª—è —Ç—É—Ä–µ—Ü–∫–æ–≥–æ —Ä–µ—Ä–∞–π—Ç–∞ ---
logger.info("üì¶ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å paraphraser (mT5 Turkish)...")
tokenizer = AutoTokenizer.from_pretrained("boun-tabi/mT5-paraphraser-turkish")
model = AutoModelForSeq2SeqLM.from_pretrained("boun-tabi/mT5-paraphraser-turkish")

def paraphrase_turkish(text):
    logger.info("üîÅ –£–Ω–∏–∫–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ mT5...")
    inputs = tokenizer.encode(text, return_tensors="pt", truncation=True, max_length=512)
    outputs = model.generate(inputs, max_length=512, num_beams=4, temperature=0.8)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# --- Telegram –æ—Ç–ø—Ä–∞–≤–∫–∞ ---
def send_to_telegram(text):
    url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
    payload = {
        "chat_id": TELEGRAM_CHAT_ID,
        "text": text,
        "parse_mode": "HTML"
    }
    try:
        response = requests.post(url, data=payload, timeout=10)
        if response.status_code == 200:
            logger.success("‚úÖ –°–æ–æ–±—â–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ Telegram.")
            return True
        else:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ –≤ Telegram: {response.status_code} - {response.text}")
            return False
    except requests.RequestException as e:
        logger.error(f"–°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ –≤ Telegram: {e}")
        return False

# --- –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ø–æ–¥ Telegram HTML ---
def format_for_telegram(text, title, url):
    clean_text = re.sub(r'</?(h\d|ul|li|div|span|table|thead|tbody|tr|td|style|script)[^>]*>', '', text).strip()
    message = f"<b>{title}</b>\n\n{clean_text}\n\n<a href='{url}'>Kaynak</a>"
    return message if len(message) <= 4096 else message[:4090] + "‚Ä¶"

# --- –ü–∞—Ä—Å–∏–Ω–≥ Ajansspor ---
def parse_ajansspor_latest_news(base_url):
    logger.info(f"üîç –ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–∞: {base_url}")
    try:
        response = requests.get(base_url, timeout=15)
        response.raise_for_status()
    except requests.RequestException as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
        return None, None

    soup = BeautifulSoup(response.content, 'html.parser')
    first_card = soup.find('div', class_='card')
    if not first_card:
        logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω –±–ª–æ–∫ —Å –Ω–æ–≤–æ—Å—Ç—å—é.")
        return None, None

    link_tag = first_card.find('a', href=True)
    if not link_tag:
        logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ –Ω–∞ –Ω–æ–≤–æ—Å—Ç—å.")
        return None, None

    news_relative_link = link_tag['href']
    full_news_url = f"https://ajansspor.com{news_relative_link}"
    return get_news_details(full_news_url)

# --- –ü–∞—Ä—Å–∏–Ω–≥ –∏ —Ä–µ—Ä–∞–π—Ç –æ–¥–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏ ---
def get_news_details(news_url):
    logger.info(f"üìÑ –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç—å—é: {news_url}")
    try:
        news_response = requests.get(news_url, timeout=15)
        news_response.raise_for_status()
    except requests.RequestException as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç–∞—Ç—å–∏: {e}")
        return None, None

    soup = BeautifulSoup(news_response.content, 'html.parser')

    header_tag = soup.find('header', class_='news-header')
    title = header_tag.get_text(strip=True) if header_tag else "Ba≈ülƒ±ksƒ±z"

    article_texts = []
    article_blocks = soup.find_all('div', class_='article-content')
    for block in article_blocks:
        article_tag = block.find('article')
        if article_tag:
            for p in article_tag.find_all('p'):
                text = p.get_text(strip=True)
                if text:
                    article_texts.append(text)

    full_text = "\n".join(article_texts)

    # –†–µ—Ä–∞–π—Ç —á–µ—Ä–µ–∑ NLP
    rewritten_text = paraphrase_turkish(full_text)

    # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
    message = format_for_telegram(rewritten_text, title, news_url)
    return message, news_url

# --- –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ---
def main():
    last_link_file = 'last_link.txt'
    last_sent_link = ""
    try:
        with open(last_link_file, 'r') as f:
            last_sent_link = f.read().strip()
            logger.info(f"üìÅ –ü—Ä–µ–¥—ã–¥—É—â–∞—è —Å—Å—ã–ª–∫–∞: {last_sent_link}")
    except FileNotFoundError:
        logger.info("üìÅ –§–∞–π–ª last_link.txt –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫.")

    url = "https://ajansspor.com/kategori/16/futbol"
    message, new_link = parse_ajansspor_latest_news(url)

    if not new_link:
        logger.info("üì≠ –ù–µ—Ç –Ω–æ–≤–æ–π –Ω–æ–≤–æ—Å—Ç–∏.")
        return

    if new_link == last_sent_link:
        logger.info("‚ôªÔ∏è –¢–∞ –∂–µ —Å—Å—ã–ª–∫–∞, —á—Ç–æ –≤ –ø—Ä–æ—à–ª—ã–π —Ä–∞–∑. –ü—Ä–æ–ø—É—Å–∫.")
    else:
        logger.info(f"üöÄ –ù–æ–≤–∞—è —Å—Ç–∞—Ç—å—è –Ω–∞–π–¥–µ–Ω–∞: {new_link}")
        if send_to_telegram(message):
            with open(last_link_file, 'w') as f:
                f.write(new_link)
            logger.success("‚úÖ –°—Å—ã–ª–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")

if __name__ == "__main__":
    main()

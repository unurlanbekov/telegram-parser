import os
import requests
from bs4 import BeautifulSoup
from loguru import logger
import re
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

# --- –ß—Ç–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ ---
HF_TOKEN = os.getenv("HUGGINGFACE_TOKEN")
TELEGRAM_TOKEN = os.getenv('TELEGRAM_TOKEN')
TELEGRAM_CHAT_ID = os.getenv('TELEGRAM_CHAT_ID')

if not HF_TOKEN:
    logger.error("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç HUGGINGFACE_TOKEN.")
    exit()
if not TELEGRAM_TOKEN or not TELEGRAM_CHAT_ID:
    logger.error("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç TELEGRAM_TOKEN –∏–ª–∏ TELEGRAM_CHAT_ID.")
    exit()

# --- –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Hugging Face ---
logger.info("üì¶ –ó–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –º–æ–¥–µ–ª—å paraphraser (mT5 Turkish)...")
tokenizer = AutoTokenizer.from_pretrained(
    "boun-tabi/mT5-paraphraser-turkish",
    use_auth_token=HF_TOKEN
)
model = AutoModelForSeq2SeqLM.from_pretrained(
    "boun-tabi/mT5-paraphraser-turkish",
    use_auth_token=HF_TOKEN
)

# --- –§—É–Ω–∫—Ü–∏—è —Ä–µ—Ä–∞–π—Ç–∞ ---
def paraphrase_turkish(text):
    logger.info("üîÅ –£–Ω–∏–∫–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ mT5...")
    input_text = text.strip().replace("\n", " ")
    prefix = "paraphrase: "
    inputs = tokenizer.encode(prefix + input_text, return_tensors="pt", max_length=512, truncation=True)
    outputs = model.generate(inputs, max_length=512, num_beams=5, temperature=0.9)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# --- Telegram –æ—Ç–ø—Ä–∞–≤–∫–∞ ---
def send_to_telegram(text):
    url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
    payload = {
        "chat_id": TELEGRAM_CHAT_ID,
        "text": text,
        "parse_mode": "HTML"
    }
    try:
        response = requests.post(url, data=payload, timeout=10)
        if response.status_code == 200:
            logger.success("‚úÖ –°–æ–æ–±—â–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ Telegram.")
            return True
        else:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ –≤ Telegram: {response.status_code} - {response.text}")
            return False
    except requests.RequestException as e:
        logger.error(f"‚ùå –°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ –≤ Telegram: {e}")
        return False

# --- Telegram —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ---
def format_for_telegram(text, title, url):
    clean_text = re.sub(r'</?(h\d|ul|li|div|span|table|thead|tbody|tr|td|style|script)[^>]*>', '', text).strip()
    message = f"<b>{title}</b>\n\n{clean_text}\n\n<a href='{url}'>Kaynak</a>"
    return message if len(message) <= 4096 else message[:4090] + "‚Ä¶"

# --- –ü–∞—Ä—Å–∏–Ω–≥ Ajansspor ---
def parse_ajansspor_latest_news(base_url):
    logger.info(f"üîç –ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–∞: {base_url}")
    try:
        response = requests.get(base_url, timeout=15)
        response.raise_for_status()
    except requests.RequestException as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
        return None, None

    soup = BeautifulSoup(response.content, 'html.parser')
    first_card = soup.find('div', class_='card')
    if not first_card:
        logger.warning("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω –±–ª–æ–∫ —Å –Ω–æ–≤–æ—Å—Ç—å—é.")
        return None, None

    link_tag = first_card.find('a', href=True)
    if not link_tag:
        logger.warning("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ –Ω–∞ –Ω–æ–≤–æ—Å—Ç—å.")
        return None, None

    news_relative_link = link_tag['href']
    full_news_url = f"https://ajansspor.com{news_relative_link}"
    return get_news_details(full_news_url)

# --- –ü–æ–ª—É—á–µ–Ω–∏–µ –∏ —Ä–µ—Ä–∞–π—Ç —Å—Ç–∞—Ç—å–∏ ---
def get_news_details(news_url):
    logger.info(f"üìÑ –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç—å—é: {news_url}")
    try:
        news_response = requests.get(news_url, timeout=15)
        news_response.raise_for_status()
    except requests.RequestException as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç–∞—Ç—å–∏: {e}")
        return None, None

    soup = BeautifulSoup(news_response.content, 'html.parser')
    header_tag = soup.find('header', class_='news-header')
    title = header_tag.get_text(strip=True) if header_tag else "Ba≈ülƒ±ksƒ±z"

    article_texts = []
    article_blocks = soup.find_all('div', class_='article-content')
    for block in article_blocks:
        article_tag = block.find('article')
        if article_tag:
            for p in article_tag.find_all('p'):
                text = p.get_text(strip=True)
                if text:
                    article_texts.append(text)

    full_text = "\n".join(article_texts)
    rewritten_text = paraphrase_turkish(full_text)
    message = format_for_telegram(rewritten_text, title, news_url)
    return message, news_url

# --- –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ---
def main():
    last_link_file = 'last_link.txt'
    last_sent_link = ""
    try:
        with open(last_link_file, 'r') as f:
            last_sent_link = f.read().strip()
            logger.info(f"üìÅ –ü–æ—Å–ª–µ–¥–Ω—è—è —Å—Å—ã–ª–∫–∞: {last_sent_link}")
    except FileNotFoundError:
        logger.info("üìÅ –§–∞–π–ª last_link.txt –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫.")

    url = "https://ajansspor.com/kategori/16/futbol"
    message, new_link = parse_ajansspor_latest_news(url)

    if not new_link:
        logger.info("üì≠ –ù–µ—Ç –Ω–æ–≤–æ–π —Å—Ç–∞—Ç—å–∏.")
        return

    if new_link == last_sent_link:
        logger.info("‚ôªÔ∏è –¢–∞ –∂–µ —Å—Ç–∞—Ç—å—è, —á—Ç–æ –∏ —Ä–∞–Ω—å—à–µ. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
    else:
        logger.info(f"üöÄ –ù–æ–≤–∞—è —Å—Ç–∞—Ç—å—è –Ω–∞–π–¥–µ–Ω–∞: {new_link}")
        if send_to_telegram(message):
            with open(last_link_file, 'w') as f:
                f.write(new_link)
            logger.success("‚úÖ –°—Å—ã–ª–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")

if __name__ == "__main__":
    main()

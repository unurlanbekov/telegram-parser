# -*- coding: utf-8 -*-
# parser_multi.py
import os
import re
import json
from typing import Optional, Tuple, List, Dict

import requests
from bs4 import BeautifulSoup
from loguru import logger

# ==========================
# –û–±—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
# ==========================
STATE_FILE = "last_links.json"
REQUEST_TIMEOUT = 20
HEADERS = {
    "User-Agent": "Mozilla/5.0",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "tr-TR,tr;q=0.9,en;q=0.8,ru;q=0.7",
    "Connection": "keep-alive",
}

# --- Telegram API –∫–ª—é—á–∏ ---
TELEGRAM_TOKEN = os.getenv('TELEGRAM_TOKEN')
TELEGRAM_CHAT_ID = os.getenv('TELEGRAM_CHAT_ID')

if not TELEGRAM_TOKEN or not TELEGRAM_CHAT_ID:
    logger.error("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç TELEGRAM_TOKEN –∏–ª–∏ TELEGRAM_CHAT_ID.")
    raise SystemExit(1)

# ==========================
# –£—Ç–∏–ª–∏—Ç—ã
# ==========================
def load_state() -> Dict[str, str]:
    if not os.path.exists(STATE_FILE):
        return {}
    try:
        with open(STATE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å {STATE_FILE}: {e}")
        return {}

def save_state(state: Dict[str, str]) -> None:
    try:
        with open(STATE_FILE, "w", encoding="utf-8") as f:
            json.dump(state, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø–∏—Å–∞—Ç—å {STATE_FILE}: {e}")

def send_to_telegram(text: str) -> bool:
    url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
    payload = {
        "chat_id": TELEGRAM_CHAT_ID,
        "text": text,
        "parse_mode": "HTML",
        "disable_web_page_preview": False,
    }
    try:
        response = requests.post(url, data=payload, timeout=10)
        if response.status_code == 200:
            logger.success("‚úÖ –°–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ Telegram.")
            return True
        logger.error(f"Telegram –æ—à–∏–±–∫–∞: {response.status_code} - {response.text}")
        return False
    except requests.RequestException as e:
        logger.error(f"Telegram —Å–µ—Ç—å: {e}")
        return False

def format_for_telegram(text: str, title: str, url: str) -> str:
    # –ß–∏—Å—Ç–∏–º —Ç—è–∂—ë–ª—ã–µ —Ç–µ–≥–∏ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π
    clean = re.sub(
        r'</?(h\d|div|span|table|tr|td|style|script|figure|img|source|picture)[^>]*>',
        '', text, flags=re.I
    ).strip()
    message = (
        f"<b>{title}</b>\n\n"
        f"{clean}\n\n"
        f"–ò—Å—Ç–æ—á–Ω–∏–∫: {url}\n"
        f"<a href='{url}'>Kaynak</a>"
    )
    return message[:4096]  # –ª–∏–º–∏—Ç Telegram

# ==========================
# Ajansspor
# ==========================
def parse_ajansspor_latest_news(base_url: str) -> Optional[Tuple[str, str]]:
    logger.info(f"üîç Ajansspor –ø–∞—Ä—Å–∏–Ω–≥: {base_url}")
    try:
        response = requests.get(base_url, timeout=REQUEST_TIMEOUT, headers=HEADERS)
        response.raise_for_status()
    except requests.RequestException as e:
        logger.error(f"Ajansspor: –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
        return None

    soup = BeautifulSoup(response.content, 'html.parser')

    # –ò—â–µ–º –ø–µ—Ä–≤—É—é –∫–∞—Ä—Ç–æ—á–∫—É/—ç–ª–µ–º–µ–Ω—Ç —Å –Ω–æ–≤–æ—Å—Ç—å—é
    first_card = soup.find('div', class_='card') \
                 or soup.find('article') \
                 or soup.select_one('.news-card, .slider-item, .list-item')

    if not first_card:
        logger.warning("Ajansspor: –Ω–µ –Ω–∞–π–¥–µ–Ω –±–ª–æ–∫ –Ω–æ–≤–æ—Å—Ç–∏.")
        return None

    link_tag = first_card.find('a', href=True)
    if not link_tag:
        logger.warning("Ajansspor: –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞.")
        return None

    href = link_tag['href'].strip()
    news_link = href if href.startswith('http') else f"https://ajansspor.com{href}"

    details = get_ajansspor_news_details(news_link)
    if not details:
        return None
    message, url = details
    return message, url

def get_ajansspor_news_details(news_url: str) -> Optional[Tuple[str, str]]:
    logger.info(f"üìÑ Ajansspor —Å—Ç–∞—Ç—å—è: {news_url}")
    try:
        resp = requests.get(news_url, timeout=REQUEST_TIMEOUT, headers=HEADERS)
        resp.raise_for_status()
    except requests.RequestException as e:
        logger.error(f"Ajansspor: –æ—à–∏–±–∫–∞ —Å—Ç–∞—Ç—å–∏: {e}")
        return None

    soup = BeautifulSoup(resp.content, 'html.parser')

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫ (–Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤)
    title_tag = soup.find('header', class_='news-header') \
                or soup.find('h1') \
                or soup.find('title')
    title = title_tag.get_text(strip=True) if title_tag else "Ba≈ülƒ±ksƒ±z"

    # –ö–æ–Ω—Ç–µ–Ω—Ç: –í–°–ï h1‚Äìh6 –∏ p –≤–Ω—É—Ç—Ä–∏ —Ä–∞–∑—É–º–Ω—ã—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤
    content: List[str] = []
    containers = soup.select('div.article-content, div.news-detail, article, main, section')
    if not containers:
        containers = [soup]
    for block in containers:
        for tag in block.find_all(re.compile(r'^(h[1-6]|p)$')):
            t = tag.get_text(" ", strip=True)
            if t:
                content.append(t)

    full_text = "\n\n".join(content).strip()
    if not full_text:
        logger.warning("‚ùå Ajansspor: —Å—Ç–∞—Ç—å—è –ø—É—Å—Ç–∞.")
        return None

    message = format_for_telegram(full_text, title, news_url)
    return message, news_url

# ==========================
# Anadolu Ajansƒ± (AA Spor)
# ==========================
AA_SPORTS_URL = "https://www.aa.com.tr/tr/spor"
AA_BLACKLIST_SNIPPETS = [
    "AA'nƒ±n WhatsApp kanallarƒ±na",
    "Bu haberi payla≈üƒ±n",
    "AA Haber Akƒ±≈ü Sistemi (HAS)",
    "Anadolu Ajansƒ± web sitesinde",
]

# –†–∞–∑—Ä–µ—à–∞–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ URL –∏ —Ä–∞–∑–Ω—ã–µ —Å–ø–æ—Ä—Ç-–ø–æ–¥—Ä–∞–∑–¥–µ–ª—ã
AA_LINK_RE = re.compile(
    r"^(?:https?://(?:www\.)?aa\.com\.tr)?"
    r"/tr/(?:spor|futbol|basketbol|voleybol|dunyadan-spor)"
    r"/[a-z0-9\-]+/\d+/?$",
    re.I,
)

def _normalize_aa_url(href: str) -> str:
    href = href.strip()
    return href if href.startswith("http") else "https://www.aa.com.tr" + href

def pick_latest_aa_article_url() -> Optional[str]:
    """–ë–µ—Ä—ë–º –ü–ï–†–í–£–Æ –ø–æ–¥—Ö–æ–¥—è—â—É—é —Å—Å—ã–ª–∫—É —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–ø–æ—Ä—Ç–∞ (–ø–æ DOM-–ø–æ—Ä—è–¥–∫—É)."""
    logger.info(f"üîç –ó–∞–≥—Ä—É–∂–∞—é –ª–µ–Ω—Ç—É AA Spor: {AA_SPORTS_URL}")
    try:
        resp = requests.get(AA_SPORTS_URL, timeout=REQUEST_TIMEOUT, headers=HEADERS)
        resp.raise_for_status()
    except requests.RequestException as e:
        logger.error(f"AA Spor (index): {e}")
        return None

    soup = BeautifulSoup(resp.content, "html.parser")

    # 1) –°—Ç—Ä–æ–≥–∏–π –º–∞—Ç—á –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É ‚Äî –≤ DOM-–ø–æ—Ä—è–¥–∫–µ
    for a in soup.find_all("a", href=True):
        href = a["href"].strip()
        if AA_LINK_RE.match(href):
            url = _normalize_aa_url(href)
            logger.debug(f"AA match (strict): {url}")
            return url

    # 2) –§–æ–ª–±—ç–∫ ‚Äî –ª—é–±–∞—è /tr/.../<id>, –≥–¥–µ –≤ –ø—É—Ç–∏ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è 'spor'
    for a in soup.find_all("a", href=True):
        href = a["href"].strip()
        if "/tr/" in href and re.search(r"/\d+/?$", href) and "spor" in href:
            url = _normalize_aa_url(href)
            logger.debug(f"AA match (fallback): {url}")
            return url

    # 3) –ï—â—ë –æ–¥–Ω–∞ –ø–æ–ø—ã—Ç–∫–∞ ‚Äî —Ç–∏–ø–∏—á–Ω—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã
    for sel in (".news", ".card", "article", ".list", ".content", ".headline"):
        for a in soup.select(f"{sel} a[href]"):
            href = a.get("href", "").strip()
            if AA_LINK_RE.match(href):
                url = _normalize_aa_url(href)
                logger.debug(f"AA match (selector {sel}): {url}")
                return url

    logger.warning("AA Spor: –Ω–µ –Ω–∞—à–ª–∏ —Å—Å—ã–ª–∫—É –Ω–∞ —Å—Ç–∞—Ç—å—é –Ω–∞ –∏–Ω–¥–µ–∫—Å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ.")
    return None

def extract_aa_article(url: str) -> Optional[Tuple[str, str]]:
    """–ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É AA: –∑–∞–≥–æ–ª–æ–≤–æ–∫ + –≤—Å–µ H1..H6 –∏ P."""
    logger.info(f"üìÑ –ó–∞–≥—Ä—É–∂–∞—é AA —Å—Ç–∞—Ç—å—é: {url}")
    try:
        r = requests.get(url, timeout=REQUEST_TIMEOUT, headers=HEADERS)
        r.raise_for_status()
    except requests.RequestException as e:
        logger.error(f"AA —Å—Ç–∞—Ç—å—è: {e}")
        return None

    soup = BeautifulSoup(r.content, "html.parser")

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    title = None
    h1 = soup.find("h1")
    if h1:
        t = h1.get_text(" ", strip=True)
        if t:
            title = t
    if not title:
        og = soup.find("meta", property="og:title")
        if og and og.get("content"):
            title = og["content"].strip()
    if not title:
        ttag = soup.find("title")
        title = ttag.get_text(strip=True) if ttag else "Ba≈ülƒ±ksƒ±z"

    # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä ‚Äî –ø–µ—Ä–≤—ã–π, –≥–¥–µ –µ—Å—Ç—å H/P
    candidates = [
        "div.detay-icerik",
        "div.print",
        'div[itemprop="articleBody"]',
        "div.article",
        "article",
        "main",
        "section.content",
        "section#content",
    ]
    body = None
    for sel in candidates:
        node = soup.select_one(sel)
        if node and (node.find(re.compile(r"^h[1-6]$")) or node.find("p")):
            body = node
            break
    if body is None:
        body = soup

    parts: List[str] = []
    for tag in body.find_all(["h1","h2","h3","h4","h5","h6","p"], recursive=True):
        classes = " ".join(tag.get("class", [])) if tag.has_attr("class") else ""
        if re.search(r"(share|sosyal|whatsapp|promo|related|cookie|benzerHaberler|subscription|banner)", classes, re.I):
            continue
        text = tag.get_text(" ", strip=True)
        if not text:
            continue
        if any(bad.lower() in text.lower() for bad in AA_BLACKLIST_SNIPPETS):
            continue
        parts.append(text)

    if not parts:
        # –∫—Ä–∞–π–Ω–∏–π —Å–ª—É—á–∞–π ‚Äî –≤—Å–µ <p> –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É
        for p in soup.find_all("p"):
            t = p.get_text(" ", strip=True)
            if t and not any(bad.lower() in t.lower() for bad in AA_BLACKLIST_SNIPPETS):
                parts.append(t)

    content = "\n\n".join(parts).strip()
    if not content:
        logger.warning("AA: —Å—Ç–∞—Ç—å—è –ø—É—Å—Ç–∞—è –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.")
        return None

    return title, content

# ==========================
# –ì–ª–∞–≤–Ω–∞—è
# ==========================
def main():
    # –í–∫–ª—é—á–∏–º –ø–æ–¥—Ä–æ–±–Ω—ã–µ –ª–æ–≥–∏ –Ω–∞ stdout
    logger.remove()
    logger.add(lambda m: print(m, end=""), level="DEBUG")

    state = load_state()
    total_sent = 0

    # --- Ajansspor ---
    try:
        ajans_url = "https://ajansspor.com/kategori/16/futbol"
        key = "ajansspor"
        res = parse_ajansspor_latest_news(ajans_url)
        if res:
            message, new_link = res
            if state.get(key) != new_link:
                logger.info(f"üöÄ Ajansspor –Ω–æ–≤–∞—è: {new_link}")
                if send_to_telegram(message):
                    state[key] = new_link
                    save_state(state)
                    total_sent += 1
            else:
                logger.info("‚ôªÔ∏è Ajansspor —É–∂–µ –ø—É–±–ª–∏–∫–æ–≤–∞–ª–∞—Å—å.")
        else:
            logger.info("üì≠ Ajansspor: –Ω–æ–≤–æ—Å—Ç—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
    except Exception as e:
        logger.exception(f"Ajansspor: –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")

    # --- AA Spor ---
    try:
        key = "aa_spor"
        url = pick_latest_aa_article_url()
        if url:
            if state.get(key) == url:
                logger.info("‚ôªÔ∏è AA —É–∂–µ –ø—É–±–ª–∏–∫–æ–≤–∞–ª–∞—Å—å.")
            else:
                parsed = extract_aa_article(url)
                if parsed:
                    title, content = parsed
                    message = format_for_telegram(content, title, url)
                    logger.info(f"üöÄ AA –Ω–æ–≤–∞—è: {url}")
                    if send_to_telegram(message):
                        state[key] = url
                        save_state(state)
                        total_sent += 1
                else:
                    logger.info("üì≠ AA: –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å —Å—Ç–∞—Ç—å—é.")
        else:
            logger.info("üì≠ AA: —Å—Å—ã–ª–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
    except Exception as e:
        logger.exception(f"AA: –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")

    if total_sent == 0:
        logger.info("üì≠ –ù–µ—Ç —Å–≤–µ–∂–∏—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏–π.")
    else:
        logger.success(f"‚úÖ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π: {total_sent}")

if __name__ == "__main__":
    main()
